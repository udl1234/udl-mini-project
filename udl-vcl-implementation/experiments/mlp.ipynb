{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from models.mlp import MLP, train_mlp, test_mlp\n",
    "\n",
    "from tasks.permuted_mnist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = permute_mnist(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(784, [100], 10).to(device)\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.000)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, test_loaders):\n",
    "    scores = []\n",
    "    for loader in test_loaders:\n",
    "        scores.append(test_mlp(model, loader, device))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MLP(28 * 28, [100], 10).to(device)\n",
    "optimizer = SGD(model1.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "sample_tmp = 1\n",
    "\n",
    "task0_train, task0_test = tasks[0]\n",
    "\n",
    "train_loader = DataLoader(task0_train, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(task0_test, batch_size=128, shuffle=False)\n",
    "\n",
    "train_mlp(model1, train_loader, device, optimizer, criterion, 10)\n",
    "\n",
    "acc = get_scores(model1, [test_loader])\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mlp import MLP2\n",
    "\n",
    "\n",
    "model1 = MLP2(28 * 28, [100], 10).to(device)\n",
    "optimizer = SGD(model1.parameters(), lr=0.01, momentum=0.9, weight_decay=0.000)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "sample_tmp = 1\n",
    "\n",
    "task0_train, task0_test = tasks[0]\n",
    "\n",
    "train_loader = DataLoader(task0_train, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(task0_test, batch_size=128, shuffle=False)\n",
    "\n",
    "train_mlp(model1, train_loader, device, optimizer, criterion, 5, False)\n",
    "\n",
    "acc = get_scores(model1, [test_loader])\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mlp import MLP2\n",
    "\n",
    "\n",
    "model1 = MLP2(28 * 28, [100], 10).to(device)\n",
    "optimizer = SGD(model1.parameters(), lr=0.01, momentum=0.9, weight_decay=0.000)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "sample_tmp = 1\n",
    "\n",
    "task0_train, task0_test = tasks[0]\n",
    "\n",
    "train_loader = DataLoader(task0_train, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(task0_test, batch_size=128, shuffle=False)\n",
    "\n",
    "train_mlp(model1, train_loader, device, optimizer, criterion, 5, True)\n",
    "\n",
    "acc = get_scores(model1, [test_loader])\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaders = []\n",
    "scores = []\n",
    "\n",
    "for i, task in enumerate(tasks):\n",
    "    print(f\"Training on task {i}\")\n",
    "\n",
    "    mnist_train, mnist_test = task\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        mnist_train,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        # num_workers=8,\n",
    "        # prefetch_factor=8,\n",
    "        # pin_memory=True,\n",
    "        # persistent_workers=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        mnist_test,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        # num_workers=4,\n",
    "        # prefetch_factor=4,\n",
    "        # pin_memory=True,\n",
    "        # persistent_workers=True,\n",
    "    )\n",
    "\n",
    "    test_loaders.append(test_loader)\n",
    "\n",
    "    sample = i > 0\n",
    "\n",
    "    train_mlp(model, train_loader, device, optimizer, criterion, 5, sample)\n",
    "\n",
    "    score = get_scores(model, test_loaders)\n",
    "    print(score)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average score for each task\n",
    "average_scores = [sum(score) / len(score) for score in scores]\n",
    "\n",
    "# Plot the average scores\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1, len(average_scores) + 1), average_scores, marker=\"o\", linestyle=\"-\")\n",
    "\n",
    "ax.set_xlabel(\"Number of tasks\")\n",
    "ax.set_ylabel(\"Average Accuracy\")\n",
    "ax.set_title(\"Average Model Accuracy Across Different Tasks\")\n",
    "ax.grid(True)\n",
    "\n",
    "# Ensure x-axis only shows integers\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig, ax = plt.subplots()\n",
    "colors = [\n",
    "    \"b\",\n",
    "    \"g\",\n",
    "    \"r\",\n",
    "    \"c\",\n",
    "    \"m\",\n",
    "    \"y\",\n",
    "    \"k\",\n",
    "]  # Define a list of colors for different tasks\n",
    "markers = [\n",
    "    \"o\",\n",
    "    \"v\",\n",
    "    \"^\",\n",
    "    \"<\",\n",
    "    \">\",\n",
    "    \"s\",\n",
    "    \"p\",\n",
    "    \"*\",\n",
    "    \"+\",\n",
    "    \"x\",\n",
    "]  # Define a list of markers for variety\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    ax.plot(\n",
    "        [i + 1] * len(score),\n",
    "        score,\n",
    "        marker=markers[i % len(markers)],\n",
    "        linestyle=\"-\",\n",
    "        color=colors[i % len(colors)],\n",
    "        label=f\"Task {i+1}\",\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Number of tasks\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Model Accuracy Across Different Tasks\")\n",
    "ax.grid(True)\n",
    "ax.legend(title=\"Tasks\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(outputs, targets):\n",
    "    return np.mean(outputs.argmax(dim=-1).cpu().numpy() == targets.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELBO(nn.Module):\n",
    "\n",
    "    def __init__(self, model, train_size, beta):\n",
    "        super().__init__()\n",
    "        self.num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        self.beta = beta\n",
    "        self.train_size = train_size\n",
    "\n",
    "    def forward(self, outputs, targets, kl):\n",
    "        assert not targets.requires_grad\n",
    "        # print(F.nll_loss(outputs, targets, reduction='mean'), self.beta * kl / self.num_params)\n",
    "        return (\n",
    "            F.nll_loss(outputs, targets, reduction=\"mean\")\n",
    "            + self.beta * kl / self.num_params\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, single_head, task_id, T=10):\n",
    "    if single_head:\n",
    "        offset = 0\n",
    "        output_nodes = 10\n",
    "    else:\n",
    "        output_nodes = model.classifiers[0].out_features\n",
    "        offset = task_id * output_nodes\n",
    "\n",
    "    model.train()\n",
    "    accs = []\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        targets -= offset\n",
    "        outputs = torch.zeros(inputs.shape[0], output_nodes, T, device=device)\n",
    "\n",
    "        for i in range(T):\n",
    "            with torch.no_grad():\n",
    "                net_out = model(inputs, task_id)\n",
    "            outputs[:, :, i] = F.log_softmax(net_out, dim=-1)\n",
    "\n",
    "        log_output = torch.logsumexp(outputs, dim=-1) - np.log(T)\n",
    "        accs.append(calculate_accuracy(log_output, targets))\n",
    "\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, num_epochs, dataloader, single_head, task_id, beta, T=10, replay=False\n",
    "):\n",
    "    beta = 0 if replay else beta\n",
    "    lr_start = 1e-3\n",
    "\n",
    "    if single_head:\n",
    "        offset = 0\n",
    "        output_nodes = 10\n",
    "    else:\n",
    "        output_nodes = model.classifiers[0].out_features\n",
    "        offset = task_id * output_nodes\n",
    "\n",
    "    train_size = (\n",
    "        len(dataloader.dataset) if single_head else dataloader.sampler.indices.shape[0]\n",
    "    )\n",
    "    elbo = ELBO(model, train_size, beta)\n",
    "    optimizer = SGD(model.parameters(), lr=lr_start, momentum=0.9)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):  # tqdm(range(num_epochs)):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        for inputs, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            targets -= offset\n",
    "            outputs = torch.zeros(inputs.shape[0], output_nodes, T, device=device)\n",
    "\n",
    "            for i in range(T):\n",
    "                net_out = model(inputs, task_id)\n",
    "                outputs[:, :, i] = F.log_softmax(net_out, dim=-1)\n",
    "\n",
    "            log_output = torch.logsumexp(outputs, dim=-1) - np.log(T)\n",
    "            kl = model.get_kl(task_id)\n",
    "            loss = elbo(log_output, targets, kl)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.bbb import *\n",
    "\n",
    "model_perm = PermutedModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task0_train, task0_test = tasks[0]\n",
    "\n",
    "train_loader = DataLoader(task0_train, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(task0_test, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "train(model_perm, 10, train_loader, True, 0, 1)\n",
    "\n",
    "accuracy = predict(model_perm, test_loader, True, 0)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final(model, epochs, loader, lr=0.01):\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_kl = 0\n",
    "        total_ll = 0\n",
    "\n",
    "        for inputs, targets in loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            kl = model.kl() / len(loader.dataset)\n",
    "            ll = model.logpred(inputs, targets)\n",
    "\n",
    "            loss = kl - ll\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_kl += kl.item()\n",
    "            total_ll += ll.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss /= len(loader)\n",
    "        total_kl /= len(loader)\n",
    "        total_ll /= len(loader)\n",
    "\n",
    "        print(f\"Epoch {epoch} Loss: {total_loss} KL: {total_kl} LL: {total_ll}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_final(model, loader):\n",
    "    model.eval()\n",
    "    accs = []\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        accs.append(calculate_accuracy(outputs, targets))\n",
    "\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mfvi import *\n",
    "\n",
    "model_final = MFVI(28 * 28, [100, 100], 10).to(device)\n",
    "\n",
    "task0_train, task0_test = tasks[0]\n",
    "\n",
    "train_loader = DataLoader(task0_train, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(task0_test, batch_size=128, shuffle=False)\n",
    "\n",
    "train_final(model_final, 10, train_loader)\n",
    "\n",
    "accuracy = test_final(model_final, test_loader)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertainty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
